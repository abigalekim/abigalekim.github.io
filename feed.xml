<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://abigalekim.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://abigalekim.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-27T22:22:47+00:00</updated><id>https://abigalekim.github.io/feed.xml</id><title type="html">blank</title><subtitle>Abigale Kim&apos;s personal website. </subtitle><entry><title type="html">A Subset of Database Systems (and Why They’re Interesting), Part 1</title><link href="https://abigalekim.github.io/interestingdbs1/" rel="alternate" type="text/html" title="A Subset of Database Systems (and Why They’re Interesting), Part 1"/><published>2023-06-10T00:00:00+00:00</published><updated>2023-06-10T00:00:00+00:00</updated><id>https://abigalekim.github.io/interestingdbs1</id><content type="html" xml:base="https://abigalekim.github.io/interestingdbs1/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>As I’m learning about many different, very interesting, fun and zesty database systems, I’ve realized that it’s difficult to realize what exactly is important about these database systems from a technical perspective. Of course, my eventual goal is to have the wisdom of an experienced databases researcher (e.g. my <a href="https://www.cs.cmu.edu/~pavlo/">advisor</a>) and innately understand the important features of a database system after a quick skim of its documentation and source code. However, I’m obviously not there yet, and I thought this would be potentially insightful and fun to create as I learned about the significance of cool database systems!</p> <h2 id="postgresql">PostgreSQL</h2> <p>Development of the PostgreSQL database management system began in the mid-1980s, and was an effort primarily led by MIT professor Mike Stonebreaker, a man so cool he’s got his own <a href="https://en.wikipedia.org/wiki/Michael_Stonebraker">Wikipedia article</a>. Originally, PostgreSQL was designed to be a one-size-fits-all database system. PostgreSQL is the pioneering database system of supporting abstract data types: both complex objects (e.g. nested tuples) and user defined data types (UDTs). PostgreSQL also has an thriving extension ecosystem, with its support for UDTs, user defined functions, user defined aggregates, and a variety of hooks (essentially, function pointers) that are used to overwrite various components of the PostgreSQL DBMS. Outside of extensibility, PostgreSQL also provides a rich set of features, including a variety of indexes, parallel execution support, and multi-version concurrency control*. However, it’s important to note that Andy Pavlo attacks PostgreSQL’s MVCC in another <a href="https://ottertune.com/blog/the-part-of-postgresql-we-hate-the-most/">blog post</a>. Lastly, PostgreSQL has a vibrant open source community of maintainers and feature developers. PostgreSQL’s impact on the databases community is incredible, as many prevailing ideas of DBMS development and implementation today are based on PostgreSQL.</p> <p>*: Multi-version concurrency control: a method of providing concurrent access to a database by storing multiple versions of the database state.</p> <h2 id="snowflake">Snowflake</h2> <p>Snowflake is a powerful, state-of-the-art online analytical processing (OLAP) data warehouse, built with cloud technology from the ground up. For instance, Snowflake uses AWS S3 as its persistent storage layer, and sets of AWS EC2 instances as its compute layer. One of Snowflake’s main goals is supporting high elasticity. To support elasticity, Snowflake separates their compute layer, persistent storage layer, and ephemeral storage layer, where the ephemeral storage layer stores intermediate data used during compute. All of these layers are individually scalable. For example, if the query planner determines that a large amount of data needs to be processed, the compute layer can scale appropriately by allocating more worker nodes to accelerate the query performance. Snowflake also uses its ephemeral storage layer as a write-through cache for its persistent storage layer. This disaggreated storage technique allows Snowflake to take advantage of cloud storage while still being highly performant. Another one of Snowflake’s advantages is that it provides its product as-a-service, which is tech jargon for allowing customers to use a database system without the required hardware or a software installation. Ultimately, Snowflake is one of the best cloud-native, scalable OLAP DBMSs in the industry.</p> <h2 id="databricks-lakehouse">Databricks Lakehouse</h2> <p>Databricks Lakehouse is a data management platform that combines techniques used in the design of both data warehouses and data lakes. Lakehouse includes four layers in their software stack, which are:</p> <ol> <li>Cloud Storage Layer: Users can choose their own cloud storage provider to persistently store their data, such as Amazon Web Service’s S3 or Google Cloud Storage.</li> <li>Data Management Layer: Lakehouse uses Delta Lake, an <a href="https://github.com/delta-io/delta">open-source</a> data management platform that provides different features such as ACID transactions, data versioning, and metadata support, in order to manage their raw storage layer.</li> <li>Execution Engine Layer: This layer is responsible for executing queries in Lakehouse. It consists of runtime clusters, that contain two types of nodes, a driver node that directs other nodes and directs execution, and executor nodes that conduct data processing. The nodes are implemented as cloud computing VMs (e.g. AWS EC2 instances). The driver runs query planning, optimizing, and scheduling tasks, while each of the executor nodes run a single threaded execution engine called Photon, which conducts a series of data processing tasks.</li> <li>User Interface Layer: Databricks provides a UI layer for users to submit SQL queries, or interface with Lakehouse by using the Apache Spark Dataframes API.</li> </ol> <p>One of the newer developments of the Lakehouse product is Photon, a single-threaded vectorized execution engine that was developed for high-performance query processing. Photon is a pull-based query engine, which means that its implementation pulls data from operators. One of the reasons that Photon is incredibly fast is because of its usage of precompiled execution kernels, which are vectorized C++ functions that execute loops over a batch of data. These functions can be specialized via C++ templates. Ultimately, Photon has helped Databricks be the current top performer of the official 100TB TPC-DS benchmark.</p> <p>Note: It is technically possible to run Databricks without Photon’s data acceleration. However, I only discuss the Databricks Lakehouse platform with Photon enabled.</p> <h2 id="google-spanner">Google Spanner</h2> <p>Spanner, a globally distributed, NewSQL DBMS developed by Google Cloud starting in 2007, is significant in the databases research sphere because it offers both a strict serializability and external consistency guarantee, without sacrificing performance, on a very large scale. In other words, while dealing with transactions involving trillions of rows and hundreds of datacenters, Spanner manages to globally order transactions and guarantee that the results of these transactions reflect their global order. It achieves this guarantee by providing globally-meaningful commit timestamps to each transaction, which reflect the serialization order. The transaction timestamps reflect the following invariant: if the transaction T2 starts after T1 commits, then the commit timestamp for T1 is lower than T2. Spanner implements this invariant via the TrueTime API, which contains one main method, <code class="language-plaintext highlighter-rouge">TT.now()</code>, which returns an interval of the global time of when it was invoked. TrueTime is implemented by a set of time master machines per datacenter and a timeslave daemon per machine. The time master machines are equipped either with GPS receivers or atomic clocks, and serve to collect the global time when polled. The timeslave daemon determines the time by polling different masters, ejecting liars and faulty clocks, and then, syncing its local machine clock to the time determined by the masters. Spanner implements multi-version concurrency control, so its read-only transactions are implemented by collecting the timestamp and reading the data at that specific timestamp. Read-write transactions are synchronized via 2PL, where timestamps are grabbed after the write locks have been obtained. Overall, Spanner’s strict serializability guarantee and the methods used to implement it have made a significant impact on the databases research sphere.</p> <h2 id="amazon-dynamodb">Amazon DynamoDB</h2> <p>DynamoDB is a highly available key-value store released in 2012 by Amazon Web Services. Two of its main goals are to provide 24/7 availability and meet its services’ stringent latency requirements. To maintain its high availability and performance, DynamoDB sacrifices a true consistency guarantee, opting instead for an eventual consistency guarantee, which means that all updates will eventually be reflected onto all the replica nodes in its distributed system. DynamoDB is a distributed system, and partitions its data across multiple nodes using a consistent hashing technnique. However, the original consistent hashing algorithm randomly assigns the position for each node on the ring, which results in a non-uniform data and load distribution. To combat this, DynamoDB assigns each node in the consistent hashing ring to multiple positions, which provides incremental scalabilty. DynamoDB also supports highly concurrent, never failing writes by keeping multiple versions of immutable update data at a time, then using vector clocks to order these updates. It also reconciles these versions back into one authoritative version during reads. Lastly, DynamoDB employs several techniques to remain fault tolerant, including a “sloppy quorum” protocol to execute distributed <code class="language-plaintext highlighter-rouge">get()</code> and <code class="language-plaintext highlighter-rouge">put()</code> operations in the case of temporary failures and <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle trees</a> to reconstruct database state in the case of permament failures. DynamoDB has resulted in high research impact on both the distributed system and database system fields, and won a SigOps Hall of Fame award in 2017. Personally, I also adore DynamoDB as a system for taking a very specific set of design constraints and goals, and creating a system that meets their needs.</p> <h2 id="duckdb">DuckDB</h2> <p>DuckDB is an embeddable OLAP DBMS developed by researchers at Centrum Wiskunde &amp; Informatica (CWI), a computer science research institute in the Netherlands. It aims to provide a solution for embeddable analytical data management, which is useful in the fields of data analytics and edge computing. One of the key features of DuckDB is their customized vector format. DuckDB uses several different vector representations, which allow it to store a compressed physical version of data, and push compressed data through their execution engine. <img src="/assets/img/duckdbvectors.png" alt=""/> DuckDB’s push-based execution engine is designed to support high amounts of parallelism. For instance, AWS instances running DuckDB can use up to 192 cores. In their implementation, their <code class="language-plaintext highlighter-rouge">Source</code> (getting data) and <code class="language-plaintext highlighter-rouge">Sink</code> (combining results from operators) methods are parallelism aware, but the operator methods themselves are not parallelism aware. One advantage of this is that handling the movement of data in a central location allows for optimizations. One example of this is scan sharing, implemented by pushing the results of operators into multiple sinks. DuckDB also uses a <a href="https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf">morsel-driven</a> parallelism model, adapted from TUM’s database system HyPeR. Overall, DuckDB’s impact on the industry is very noticeable, especially given how new of a system it is!</p> <h2 id="conclusion">Conclusion</h2> <p>Ultimately, this turned into a “things I personally found interesting about database systems I’ve seen recently” post as opposed to “this are the de-facto features that make these databases systems special” post; however, I like this style better! If you read the post, I hope you learned something, and feel free to give me any feedback or let me know if I got anything wrong.</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>I would like to thank my friend <a href="https://codetector.org/">Yaotian Feng</a> for reading through this article and providing helpful feedback!</p> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/pdf/1901.01973.pdf">Looking Back at Postgres</a></li> <li><a href="https://www.postgresql.org/docs/15/index.html">Postgres 15 Docs</a></li> <li><a href="https://15721.courses.cs.cmu.edu/spring2023/papers/02-modern/vuppalapati-nsdi22.pdf">Building An Elastic Query Engine on Disaggregated Storage</a></li> <li><a href="https://15721.courses.cs.cmu.edu/spring2023/papers/21-snowflake/p215-dageville-snowflake.pdf">The Snowflake Elastic Data Warehouse</a></li> <li><a href="https://15721.courses.cs.cmu.edu/spring2023/papers/20-databricks/sigmod_photon.pdf">Photon: A Fast Query Engine for Lakehouse Systems</a></li> <li><a href="https://15721.courses.cs.cmu.edu/spring2023/papers/20-databricks/p975-armbrust.pdf">Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores</a></li> <li><a href="https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf">Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics</a></li> <li><a href="https://cloud.google.com/spanner">Google Cloud Spanner Docs</a></li> <li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf">Spanner: Google’s Globally-Distributed Database</a></li> <li><a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a></li> <li><a href="https://www.usenix.org/system/files/atc22-elhemali.pdf">Amazon DynamoDB: A Scalable, Predictably Performant, and Fully Managed NoSQL Database Service</a></li> <li><a href="https://mytherin.github.io/papers/2019-duckdbdemo.pdf">DuckDB: an Embeddable Analytical Database</a></li> <li><a href="https://15721.courses.cs.cmu.edu/spring2023/schedule.html">Andy Pavlo’s 15-721 Spring 2023 Lectures</a></li> </ul>]]></content><author><name></name></author><category term="databases"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Development of OLAP Systems in the 21st Century</title><link href="https://abigalekim.github.io/adbfinal/" rel="alternate" type="text/html" title="Development of OLAP Systems in the 21st Century"/><published>2023-06-06T00:00:00+00:00</published><updated>2023-06-06T00:00:00+00:00</updated><id>https://abigalekim.github.io/adbfinal</id><content type="html" xml:base="https://abigalekim.github.io/adbfinal/"><![CDATA[<p>Although this is my final essay for Advanced Database Systems, I thought it was pretty good blog content, so I’m placing it here! Notably, I also wrote this essay without using ChatGPT.</p> <h2 id="introduction">Introduction</h2> <p>Database management systems (“DBMSs”) designed for online analytical processing (“OLAP”) workloads have become an ubiquitous presence in the technological sphere due to the rise of fields such as big data analytics and data science. One main goal of the designers of OLAP systems is to make them as highly performant as possible. The various techniques applied to achieve this goal has resulted in a myriad of different implementation methods to accelerate query execution for OLAP workloads. In this essay, we cover the major milestones and key developments in query execution for OLAP workloads over the past twenty-three years. We conclude by discussing the optimization that developers should consider first when building a new OLAP DBMS.</p> <h2 id="key-developments-in-query-execution">Key Developments in Query Execution</h2> <h3 id="early-columnar-storage-database-systems">Early Columnar Storage Database Systems</h3> <p>In the early 2000s, data warehouses such as Sybase IQ and Addamark showed the benefits of using a column store architecture. A few years later, MonetDB and C-Store, two impactful column-store systems developed in the early 2000s, were released.</p> <p>MonetDB is a column store relational database system that was developed by Centrum Wiskunde &amp; Informatica (CWI) in the early 2000s. The first version of MonetDB used the materialization query execution model. Unfortunately, using the materialization model caused the performance of large queries to be bottlenecked by memory bandwidth. In 2005, MonetDB rewrote their query engine (called ``X100”) to use a vectorized query model [2]. This allowed their execution engine to reap the benefits of the Volcano model, while not sacrificing the important compiler optimizations (e.g. loop pipelining, which takes advantage of a multi-core CPU by executing operations in parallel) of the materialization model. Overall, the paper on MonetDB/X100 showed that by using the vectorized query execution model, one can design an execution engine that takes advantage of the parallelism offered by multi-core processors.</p> <p>C-Store [10] is a column-store relational database system that was developed by Massachusetts Institute of Technology, Brandeis University, University of Massachusetts Boston, and Brown University, and released in 2005. The design of C-Store is optimized for a shared-nothing distributed architecture. C-Store also internally compresses its data, which will be discussed in the next section.</p> <h3 id="compression-in-column-store-dbmss">Compression in Column Store DBMSs</h3> <p>The compression of data is important for query execution acceleration. This is because it improves I/O performance by reducing seek times/data transfer rates and increases the buffer hit rate.</p> <p>As mentioned in the previous section, C-Store is a column-store relational DBMS that compresses its data internally. In 2006, Daniel Abadi et. al. evaluate the compression methods used in C-Store [1], which include null suppression, dictionary encoding, run-length encoding, bit vector encoding, and several heavyweight compression schemes, including Lempel-Ziv encoding, Huffman encoding, and Arithmetic encoding. They also create a decision tree for the use cases of different compression schemes in OLAP DBMSs. The paper finds that choosing the correct compression scheme depends not only on data properties, but also on the query workload.</p> <h3 id="taking-advantage-of-parallelism">Taking Advantage of Parallelism</h3> <p>The increased availability and performance of multi-core hardware in the 2010s led database systems developers to attempt to take advantage of it as best they could. The results of their efforts include key advancements in the fields of query scheduling, query vectorization, query compilation, and parallel join algorithms. Ultimately, these key developments led to increased query performance in their respective OLAP systems.</p> <h4 id="query-scheduling">Query Scheduling</h4> <p>One of the main developments of the query scheduling field is developing non uniform memory access (NUMA) aware query schedulers. Utilizing multi-processors that use NUMA correctly can lead to significant performance benefits, but it requires coordinating memory access and processing tasks between each of the different cores. If this is done incorrectly, since accessing memory over the interconnect is more expensive than accessing local memory, the query performance will be significantly worse. In 2014, Viktor Leis et al. present a NUMA aware, morsel-driven query scheduler [3], implemented in HyPeR, Technical University of Munich’s (TUM) academic DBMS. In their scheduling implementation, the workers perform cooperative scheduling by pulling tasks off a shared task queue. The workers will first try to execute tasks that operate on the morsels in its local memory, but if that is not possible, they just pull the next task off the queue. HyPeR also implements work stealing, so that workers that are idle can steal tasks from workers who are taking too long to execute the tasks they have already removed from the queue. In 2021, TUM published a paper on their improved lock-free, self-tuning, NUMA aware, morsel-driven stride scheduler [12]. This scheduler is implemented in Umbra, HyPeR’s successor. Although it uses some of the techniques mentioned in their 2014 paper, there are some key differences. First, the scheduler is thread-local, which means that each thread determines which active tasks it should execute by examining its own metadata. Second, this scheduler is self-tuning, which means that it simulates its own execution, then adjusts its own hyper-parameters by keeping track of query priorities.</p> <h4 id="query-compilation">Query Compilation</h4> <p>One important technique to increase the speed of OLAP queries is query compilation, where the query engine generates code either via just-in-time compilation or transpilation. Query code generation decreases the instruction count of the code ran, while doing the same amount of work. One example of research that shows that query compilation is an effective optimization technique is Thomas Neumann’s 2015 VLDB paper, where he presents a new data-centric compilation strategy that compiles a query into LLVM machine code [6]. His compilation strategy focuses on pushing data between operators so that tuples can remain in CPU registers for as long as possible. Overall, this strategy reduces the raw CPU costs of query processing, and the performance of the implementation (done in HyPeR) of this technique rivals hand-optimized code.</p> <p>Meta’s Velox, an open source C++ execution engine acceleration library, also uses query compilation [7]. Velox uses query compilation in their vectorized expression evaluation engine. This engine is used in the evaluation of filter and projection operators, and in the implementation of predicate pushdown for their table scan operator. The engine takes a list of expression trees and applies several compiler optimizations to them, including common sub-expression elimination and constant folding. It also reorders Boolean expressions in a conjunctive expression by order of selectivity, and uses memoization to reuse compiled objects as needed. The expression evaluation engine returns a compiled executable.</p> <p>Although both HyPeR and Velox use query compilation to increase the speed of query execution, and both use a push-based query processing model, their methods and extents of using query compilation are very different. Since Velox only implements a execution engine library, the extend to which query compilation is embedded in the system is not extreme, and it is only used in a limited context. On the other hand, HyPeR’s execution engine is rewritten to embrace the new data-centric compilation strategy.</p> <h4 id="query-vectorization">Query Vectorization</h4> <p>Essentially every OLAP DBMS uses vectorized query execution with Single Instruction/Multiple Data (SIMD). Using SIMD improves the performance of OLAP DBMSs because when executing analytical queries, SIMD enables the execution engine to process data in parallel. For example, in 2015, Polychroniou et al. present implementations of selection scans, hash tables, partitioning, sorting, and joins using advanced SIMD operators [8]. For instance, they implement selection scans by evaluating the predicates in parallel, storing a bit mask of the qualifying offsets, and then propagating the qualifying tuples immediately to the output vector. They show that their operators accelerate query execution on both mainstream CPUs and a MIC-based Xeon Phi co-processor, compared to both scalar and compiler-vectorized implementations of their database operators.</p> <h3 id="the-effects-of-cloud-platforms">The Effects of Cloud Platforms</h3> <p>Cloud computing rose to prominence in the 2010s, and cloud platforms now offer essentially ``unlimited” storage and compute on demand. Cloud computing has allowed database vendors to offer their products as a service, which means that the database is entirely stored and accessed through the cloud. Two examples of OLAP databases offered as a service are Amazon Redshift and Snowflake. Cloud computing prominence has also resulted in the usage of cloud object stores as a data storage backend. When using a cloud object store as a storage backend, the DBMS stores the database contents in large, immutable files in a service such as Amazon S3, Azure Blob, or Google Cloud Storage. Some benefits of this method include not having to write a storage system, having almost unlimited scalable storage, and being capable of separating one’s storage and compute resources, the latter of which will be discussed in the next section.</p> <h3 id="distributed-storage-and-compute">Distributed Storage and Compute</h3> <p>Prior to the existence of cloud platforms, most distributed OLAP systems had shared-nothing system architectures. For instance, both C-Store [10] and Greenplum [4] use a shared-nothing architecture. To take advantage of modern storage and compute resources presented by the cloud computing movement, DBMSs now use a distributed compute and storage layer, which scale independently of each other. Two modern OLAP systems that use this technique are Snowflake and Google Dremel.</p> <p>Snowflake describes their process of disaggregating storage and compute in their 2020 NSDI paper [11]. They reveal that they separate compute, ephemeral storage (intermediate data used in the compute process), and persistent storage. Snowflake uses the abstraction of a virtual warehouse (VW) to represent compute. A VW is a set of AWS EC2 instances that run a set of queries. The EC2 instances in a VW share a distributed local storage system used for ephemeral storage. Snowflake uses AWS S3 for persistent storage. By adding the ephemeral storage layer, the Snowflake system does not sacrifice performance while maintaining high levels of both scalability and availability.</p> <p>In the same year, Google’s Dremel also describes their disaggregated architecture [5]. Although Dremel used to be a shared-nothing distributed system, Dremel uses Borg, Google’s cluster management system, as compute, and a distributed replicated storage system, as storage. Dremel also extensively uses its distributed in-memory shuffle tier during query processing, which reduces memory access overhead. As a result, Dremel is significantly more scalable, faster, and capable of processing petabyte-sized tables.</p> <p>The main ideas of Snowflake’s and Dremel’s disaggregated storage/compute architectures are similar, but the implementations of them are different. Both Snowflake and Dremel have three main layers: a compute layer, which consists purely of compute nodes, an intermediate local storage layer, and a persistent storage layer. However, Snowflake uses S3 for their persistent storage layer, while Dremel uses local disks that are managed by independent servers. Additionally, Snowflake uses their intermediate storage layer for two purposes: first, it serves as a method to exchange data between compute nodes in the same VW, and second, it serves as a write-through cache for persistent data. Meanwhile, Dremel uses their intermediate storage layer to store the intermediate results of queries.</p> <h2 id="conclusion">Conclusion</h2> <p>The optimization that developers should consider first when building a new OLAP DBMS is to use a push-based query processing model. In 2011, HyPeR [6] showed that by using a push-based model, one can keep a tuple in a CPU’s registers for as long as possible, which makes good use of modern, multi-core CPUs. Additionally, both Velox [7] and DuckDB [9] use a push-based query processing model, with good results of query performance. For example, Velox shows that their query engine is on average, 6-7 times faster than the Presto Java engine. Lastly, a query execution engine based on the push-based query processing model intuitively supports parallelism. For example, DuckDB’s implementation of both <code class="language-plaintext highlighter-rouge">Source</code> and <code class="language-plaintext highlighter-rouge">Sink</code> methods, which get data and collect the results of operators, respectively, are parallelism-aware.</p> <h2 id="references">References</h2> <ul> <li>[1] Abadi, D., Madden, S., and Ferreira, M. Integrating compression and execution in column-oriented database systems. In Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data (New York, NY, USA, 2006), SIGMOD ’06, Association for Computing Machinery, p. 671–682.</li> <li>[2] Boncz, P. A., Zukowski, M., and Nes, N. MonetDB/X100: Hyper-pipelining query execution. In Conference on Innovative Data Systems Research (2005), vol. 5, pp. 225–237.</li> <li>[3] Leis, V., Boncz, P., Kemper, A., and Neumann, T. Morsel-driven parallelism: A numa-aware query evaluation framework for the many-core age. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data (New York, NY, USA, 2014), SIGMOD ’14, Association for Computing Machinery, p. 743–754.</li> <li>[4] Lyu, Z., Zhang, H. H., Xiong, G., Guo, G., Wang, H., Chen, J., Praveen, A., Yang, Y., Gao, X., Wang, A., Lin, W., Agrawal, A., Yang, J., Wu, H., Li, X., Guo, F., Wu, J., Zhang, J., and Raghavan, V. Greenplum: A hybrid database for transactional and analytical workloads. In Proceedings of the 2021 International Conference on Management of Data (New York, NY, USA, 2021), SIGMOD ’21, Association for Computing Machinery, p. 2530–2542.</li> <li>[5] Melnik, S., Gubarev, A., Long, J. J., Romer, G., Shivakumar, S., Tolton, M., Vassilakis, T., Ahmadi, H., Delorey, D., Min, S., Pasumansky, M., and Shute, J. Dremel: A decade of interactive sql analysis at web scale. Proc. VLDB Endow. 13, 12 (aug 2020), 3461–3472.</li> <li>[6] Neumann, T. Efficiently compiling efficient query plans for modern hardware. Proc. VLDB Endow. 4, 9 (jun 2011), 539–550.</li> <li>[7] Pedreira, P., Erling, O., Basmanova, M., Wilfong, K., Sakka, L., Pai, K., He, W., and Chattopadhyay, B. Velox: Meta’s unified execution engine. Proc. VLDB Endow. 15, 12 (aug 2022), 3372–3384.</li> <li>[8] Polychroniou, O., Raghavan, A., and Ross, K. A. Rethinking simd vectorization for in-memory databases. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (New York, NY, USA, 2015), SIGMOD ’15, Association for Computing Machinery, p. 1493–1508.</li> <li>[9] Raasveldt, M., and M ̈uhleisen, H. Duckdb: An embeddable analytical database. In Proceedings of the 2019 International Conference on Management of Data (New York, NY, USA, 2019), SIGMOD ’19, Association for Computing Machinery, p. 1981–1984.</li> <li>[10] Stonebraker, M., Abadi, D. J., Batkin, A., Chen, X., Cherniack, M., Ferreira, M., Lau, E., Lin, A., Madden, S., O’Neil, E., O’Neil, P., Rasin, A., Tran, N., and Zdonik, S. C-store: A column-oriented dbms. In Proceedings of the 31st International Conference on Very Large Data Bases (2005), VLDB ’05, VLDB Endowment, p. 553–564.</li> <li>[11] Vuppalapati, M., Miron, J., Agarwal, R., Truong, D., Motivala, A., and Cruanes, T. Building an elastic query engine on disaggregated storage. In Proceedings of the 17th Usenix Conference on Networked Systems Design and Implementation (USA, 2020), NSDI’20, USENIX Association, p. 449–462.</li> <li>[12] Wagner, B., Kohn, A., and Neumann, T. Self-tuning query scheduling for analytical workloads. In Proceedings of the 2021 International Conference on Management of Data (New York, NY, USA, 2021), SIGMOD ’21, Association for Computing Machinery, p. 1879–1891.</li> </ul>]]></content><author><name></name></author><category term="databases"/><summary type="html"><![CDATA[Although this is my final essay for Advanced Database Systems, I thought it was pretty good blog content, so I’m placing it here! Notably, I also wrote this essay without using ChatGPT.]]></summary></entry><entry><title type="html">Software Systems Curriculum Learning Objectives</title><link href="https://abigalekim.github.io/systemsobjectives/" rel="alternate" type="text/html" title="Software Systems Curriculum Learning Objectives"/><published>2022-11-10T00:00:00+00:00</published><updated>2022-11-10T00:00:00+00:00</updated><id>https://abigalekim.github.io/systemsobjectives</id><content type="html" xml:base="https://abigalekim.github.io/systemsobjectives/"><![CDATA[<p>Over my 3 years of being a teaching assistant for software systems classes, I’ve naturally developed quite a few opinions on the software systems education sphere.* The first of these opinions is what set of skills should be taught in a systems curriculum, where I define a systems curriculum within a university setting as the core intro systems class and a few upper division computer systems courses. I list these set of skills below.</p> <h3 id="reading-and-understanding-a-software-systems-components">Reading and understanding a software system’s components</h3> <p>Unless the student is implementing a software system from scratch, on their own, a student should learn how to read other people’s code. In fact, a significant amount of my work in the DBMS industry has been reading the codebase to understand how the moving components of existing code worked. Then, with time, after reading the code, the student should be capable of understanding the gist of how the software system works, and how the moving components of the system interact with each other. They should also be capable of reading and understanding partially created software systems.</p> <h3 id="readingwriting-documentation">Reading/writing documentation</h3> <p>One important part of understanding software systems is learning how to read documentation. It is common for a software system to contain external libraries or frameworks. These also need to be understood at least on a surface level by a student trying to understand a software system.</p> <p>Conversely, students will also need to learn how to write their own documentation for the code they eventually write. This is important because it helps other developers understand their code.</p> <h3 id="systems-design">Systems design</h3> <p>Students should learn how to design their own software systems, and features within larger, existing software systems. The skills needed to design a software system from scratch include taking a bigger idea, which is how the system is supposed to operate/a specification, and then compartmentalizing this general concept into different moving parts. Similarly, the main skill needed to design a feature for a larger, existing software system is the ability to conceptualize how the student’s feature would fit within the existing system.</p> <h3 id="systems-implementation">Systems implementation</h3> <p>Students need to know how to understand a software system specification and convert it into code that is reliable and performs well. Students will need to be capable of translating ideas into a lower-level programming language, writing comprehensive, extensive, and robust tests (both for correctness and performance), and debugging all sorts of failures in their code.</p> <h3 id="translating-ideas">Translating ideas</h3> <p>Students should learn how to translate higher level ideas into code. This includes learning how to think in different paradigms, such as functional/imperative, object-oriented programming, event-driven programming, and procedural programming. Students should also be prepared to implement software systems in different languages, depending on the purpose of the system.</p> <h3 id="writing-tests">Writing tests</h3> <p>Students will need to be capable of testing both the correctness and performance of their code. They should be capable of writing fully comprehensive and robust tests, which include attacking edge cases and stress testing. Students should also be capable of evaluating the performance (both time and resource performance) of systems software, and fixing bugs as necessary to improve their code’s performance.</p> <h3 id="debugging-skills">Debugging skills</h3> <p>Students will inevitably run into a lot of bugs when working on software systems related work. To be capable of solving these bugs, students will need to develop something I call “debugging resilience”. Essentially, debugging resilience means that when a student encounters a bug, they have a plan of action until they figure out what they cannot understand. This would be opposed to a response such as immediately asking for help without attempting to gain information, or being frozen on action items. Given the current software systems ecosystem’s shift toward parallel programming paradigms, students should also understand how to understand and debug concurrently executing code.</p> <h3 id="asking-for-help-the-right-way">Asking for help (the right way!)</h3> <p>Inevitably, in a difficult environment such as software systems development, a student will struggle. Whether it is to design, implement, or debug a feature, a student should know how to gain information about their predicament and figure out what exactly they do not know.</p> <h3 id="conclusion">Conclusion</h3> <p>This naturally raises a couple questions:</p> <ul> <li>Experience is a guaranteed way to learn all of these skills. But is there a way to design curriculum such that learning these skills is necessary to completing the curriculum?</li> <li>Can a subset of these skills be self-taught (like, without the aid of classes)? Can all of them be self-taught?</li> </ul> <p>And… that’s all for my second blog post. I’m happy to discuss thoughts on it if you email me! (Or if you know me IRL, just reach out LOL)</p> <p>* I think it’d be fun to detail these in a few blog posts, but I’m bad at allocating free time for blogging :P</p>]]></content><author><name></name></author><category term="cs-education"/><summary type="html"><![CDATA[Over my 3 years of being a teaching assistant for software systems classes, I’ve naturally developed quite a few opinions on the software systems education sphere.* The first of these opinions is what set of skills should be taught in a systems curriculum, where I define a systems curriculum within a university setting as the core intro systems class and a few upper division computer systems courses. I list these set of skills below.]]></summary></entry><entry><title type="html">Why CMU SCS?</title><link href="https://abigalekim.github.io/whycmuscs/" rel="alternate" type="text/html" title="Why CMU SCS?"/><published>2021-01-26T00:00:00+00:00</published><updated>2021-01-26T00:00:00+00:00</updated><id>https://abigalekim.github.io/whycmuscs</id><content type="html" xml:base="https://abigalekim.github.io/whycmuscs/"><![CDATA[<p>Choosing the right college to attend can be a really daunting experience, since there are so many things to consider! My hope is that this post gives some clarity as to what CMU SCS has to offer to undergrads (and what it doesn’t), and helps you make an informed decision.</p> <p>Note 1: Classes at CMU have IDs that are in the format of a 2 digit number followed by a 3 digit number. The 2 digit number represents the department (SCS’s department number is “15”, but if a class is meant only for SCS students, then the department number is “07”), and the 3 digit number is the class number. I will be referring to classes as such throughout this blog post. Additionally, you can find info about the classes that I mention by Googling “{class number} cmu”. Usually the class websites are the first results when you do this.</p> <p>Note 2: The “CS core”, mentioned a few times in this blog post, is the set of CS classes that all computer science majors take at CMU. They include:</p> <ul> <li>15-151: Mathematical Foundations for Computer Science</li> <li>15-112: Fundamentals of Programming <ul> <li>Note: If you’re a CS major, you can get credit for 15-112 either by getting a 5 on the AP CS A test, or passing a placement exam in the summer before the academic year. In this case, you would take 15-122 during your first semester.</li> </ul> </li> <li>15-122: Principles of Imperative Computation</li> <li>15-150: Principles of Functional Programming</li> <li>15-213: Introduction to Computer Systems</li> <li>15-210: Parallel and Sequential Data Structures and Algorithms</li> <li>15-251: Great Ideas in Theoretical Computer Science</li> <li>15-451: Design and Analysis of Algorithms</li> </ul> <h1 id="so-why-cmu-scs">So, why CMU SCS?</h1> <h3 id="academics">Academics</h3> <p>CMU SCS has a lot of academic opportunity to offer. There are many upper level classes offered on many different topics within CS. You may not know that computer science isn’t just Java programming (AP CS A gang!), and instead consists of a variety of fields, including computational biology, algorithms, complexity theory, programming language theory, computer systems, human computer interaction, computer graphics, artificial intelligence, machine learning, natural language processing, computer security, computer architecture–I could go on and on.</p> <p>CMU SCS provides opportunities to explore all of these subfields, both through the classes that you take, and through research. For instance, <a href="https://csd.cmu.edu/cs-and-related-undergraduate-courses">here</a> is a outdated list of the classes offered by SCS (now there’s even more being offered!). Anecdotally, I’ve found that the course offerings satisfy a large majority of the student body’s interests within CS.</p> <p>Currently, CMU SCS offers four bachelor’s degrees:</p> <ul> <li>artificial intelligence</li> <li>computational biology</li> <li>computer science (with an additional major or concentration, see next paragraph)</li> <li>human-computer interaction</li> </ul> <p>Although the computer science degree sounds very general, all students who major in computer science are required to pick a concentration (think a minor, but in a sub-field of CS), minor, or additional major to complete at CMU. Essentially, you’re required to explore at least one area of study outside the general CS curriculum. Even if this may seem like extra work, I view it as a really good thing, since it provides excellent motivation to explore your academic interests.</p> <p>You can find a list of computer science concentrations <a href="http://coursecatalog.web.cmu.edu/schools-colleges/schoolofcomputerscience/scsconcentrations/">here</a>. I’ve also compiled websites listing the minors offered per school below:</p> <ul> <li><a href="http://coursecatalog.web.cmu.edu/schools-colleges/collegeofengineering/minorsfornonengineeringstudents/">College of Engineering (CIT)</a></li> <li><a href="https://www.cmu.edu/cfa/schools-and-academic-programs/majors-and-minors/index.html">College of Fine Arts (CFA)</a></li> <li><a href="https://www.cmu.edu/dietrich/academics/degrees-majors-minors/minors.html">Dietrich College of Humanities &amp; Social Sciences</a></li> <li><a href="http://coursecatalog.web.cmu.edu/schools-colleges/melloncollegeofscience/melloncollegeofscienceminors/">Mellon College of Science</a></li> <li><a href="http://coursecatalog.web.cmu.edu/schools-colleges/schoolofcomputerscience/addlmajorsminors/">School of Computer Science</a></li> <li><a href="https://www.cmu.edu/tepper/programs/undergraduate-business/curriculum/majors-and-minors.html">Tepper School of Business</a></li> </ul> <p>If this seems like an information overload, don’t worry! Students in SCS usually do not decide on their concentration/minor/additional major quickly–it takes usually 1-2 years for most people to decide, and you can do more than one! I had a lot of trouble deciding my concentration (I chose computer systems), but I eventually found that the decision came much more naturally than expected.</p> <p>CMU SCS also offers a 5th year masters program for SCS undergraduates. This allows you to get two degrees in the span of 5 years! This masters degree allows students to expand their research horizons and prepare them for a PhD program. Typically, students in the 5th year masters degree program carry out research with a faculty advisor and take two CS PhD level classes per semester.</p> <h3 id="research-opportunities">Research Opportunities</h3> <p>Another one of CMU SCS’s strong points is research opportunities. It’s easy to get involved with research as an undergraduate, and the administrators (thank you, Tom Cortina!) make this process even easier by emailing the SCS students a large Google Doc filled with research projects every couple of months or so. I got my first research opportunity by emailing professors mentioned on this Google Doc. There are also plenty of research groups that you can join. Professors are usually very willing to allow more students into their research group. Research at CMU can be counted as independent study course credit, or you can get paid as a research assistant.</p> <p>There also is a course sequence (15-300 and 15-400) that specifically prepares SCS students for research. This sequence is usually taken in one’s junior year. One of the assignments in 15-300 is to find a research advisor and write a proposal on what you will be researching. Then, in 15-400 (Research Practicum in Computer Science), you carry out the project you proposed in 15-300.</p> <p>You can also opt to do an Undergraduate Research Thesis your senior year. This is a two semester endeavor where you work closely with a faculty advisor to carry out research in a topic of your choice. More information on both independent study and the senior thesis is <a href="https://csd.cmu.edu/cs-undergraduate-research-options">here</a>.</p> <p>Research opportunities are crucial if you are considering pursuing a PhD or research based masters degree after completing your bachelor’s. For those who don’t want to pursue post-bachelor’s education, research opportunities can also help your resume stand out to companies.</p> <h3 id="industry-opportunity">Industry Opportunity</h3> <p>Just having “CMU SCS’’ on your resume as an applicant for internships or entry-level positions makes you stand out. CMU SCS is recognized by many companies, including all of FAANG, as a top tier school. Some proprietary trading firms, such as Jane Street and Jump Trading, also recruit from CMU and pick a relatively large number of applicants. CMU SCS also distributes all student resumes so that companies can recruit you directly.</p> <p>Although I’ll admit that the computer science internship job market is a crapshoot, your chances of getting an internship or entry-level job from the company that you want are relatively high if you go to CMU SCS. Many CMU students end up interning at big tech companies (if that’s your goal) and have the opportunity to intern at smaller companies of their choosing.</p> <p>If you want to see for yourself, you can take a look at the following links:</p> <ul> <li><a href="https://www.cmu.edu/career/outcomes/post-grad-dashboard.html">CMU Post Graduation Outcomes</a></li> <li><a href="https://www.cmu.edu/career/outcomes/internship-dashboard.html">CMU Internship Outcomes</a></li> </ul> <h3 id="taing">TAing</h3> <p>TAing, or being a teaching assistant for a class, is my absolute favorite part of CMU SCS. You can be involved! You can help students! You can give back to your SCS community! You can work on course development, where you create material for the course you TA! And to top it off, TAing is a resume and grad school app booster (although that should not be the only reason you TA).</p> <p>First, there are lots of opportunities to TA. Nearly all SCS classes will take undergraduate TAs. Some (such as 15-112 and 15-122), hire more than 40 TAs a semester, with most of the core employing 20-30 TAs a semester. There are just so many spots to fill that the demand for TAs is higher than the supply.</p> <p>TAing is also a very enriching experience. You learn many soft skills from TAing, such as communication, public speaking, patience, and responsibility. Additionally, you can make a huge impact on students, befriend your fellow TAs and professor(s), and may even learn from your students (I certainly have).</p> <p>During your first 1-2 years as a SCS student, you will mostly take the CS core, and since you can only TA a class after you’ve taken it (with very few exceptions), most of your opportunities for TAing a CS class will be the core. TAing a class in the CS core mostly consists of interactions with students. The CS core is difficult, and many students need lots of support from staff in order to do well. Therefore, your main responsibilities as a core class TA include holding office hours (scheduled times to meet with students and give course support) and recitations (TA-led review sessions of material).</p> <p>However, there are some exceptions to this set of responsibilities. TAs in core classes also have the opportunity to work on course development for the class they TA. Course development responsibilities can span from creating new assignments, adding features/testing to assignments, and creating support material for students.</p> <p>Later in your undergraduate career, you will have the opportunity to take and TA upper division elective classes. Upper division classes usually offer less student support (but this is okay, since students are more prepared to take them). As a result, TA responsibility for an upper-div class shifts to include more course development and less student support.</p> <p>There are a few downsides to TAing. First, most TAs work about 10-20 hours a week, so it’s a sizable commitment in addition to classes. TAs are also not paid very much (about $10 to $13 per hour). Lastly, I would claim that TAing online is not an easy task, as communication to students often gets mangled in an online setting.</p> <p>Compared to other colleges, which mostly offer TAing opportunities to graduate students, CMU SCS is an outlier in its unusually large number of undergraduate TAs. TA experience helps students stand out on job and graduate school apps, and is ultimately an extremely rewarding experience.</p> <h3 id="tight-knit-community">Tight Knit Community</h3> <p>The SCS community is a tight-knit, loving, and supportive one. I’ve found that most of the people I meet are kind, helpful, friendly, and interesting people. Although the stereotype of CS majors is that we’re arrogant and not very friendly, I’ve found that the CS majors I meet here are pretty much the opposite. Additionally, CMU SCS is a very small program, with about 200 students per class. This makes it easier to get to know your peers.</p> <p>The SCS student community has really tried to stay in touch online. For example, there’s an active Discord server for CS and ECE majors to discuss computer science and any other topic. Organizations such as Women@SCS and SCS4All also hold student events (such as Among Us game night!).</p> <p>Most professors here are friendly too. Back when school was not online (AAAAAAH), many professors would encourage students to talk to them as they pleased. In online semesters, professors really try to connect to students. For instance, both my databases and programming language theory professors held Q&amp;A sessions after class where students could ask them anything. When I was anxious about the PL theory final, my professor even met with me individually the day before. These are just anecdotes from my online semester, but I’m certain many other SCS students have had similar experiences.</p> <h3 id="free-food-and-swag-when-campus-resumes-existing">Free Food and Swag (when campus resumes existing)</h3> <p>I’ve found that the opportunities of obtaining free food and swag (most of my wardrobe consists of free company T-shirts) are wonderful at this school. Companies lure students in to their tech talks and recruiting events with free food and swag, and many student-led events also include free food. Additionally, Catherine Copetas, one of the SCS Assistant Deans, will sometimes place cookies and other goodies, including vegetable stress balls, outside her office (her office is on the 6th floor).</p> <h1 id="why-not-cmu-scs">Why not CMU SCS?</h1> <h3 id="cost">Cost</h3> <p>The number one reason that most people do not choose CMU is cost. The estimated <a href="https://www.cmu.edu/sfs/tuition/undergraduate/index.html">cost of attendance</a> per year is $76,715. The averaged aid given is listed <a href="https://admission.enrollment.cmu.edu/pages/financial-aid">here</a>. You can subtract the estimated aid you will receive to find how much CMU approximately costs. Alternatively, you can use the <a href="https://admission.enrollment.cmu.edu/pages/net-price-calculator">Net Price Calculator</a> if you want a more specific estimate. Attending CMU is a large financial investment, especially if you do not qualify for much financial aid. You can make a good portion of the tuition back via high computer science internship salaries, but you shouldn’t count on it as a freshman/sophomore. Additionally, you will be able to pay off the cost of attendance pretty fast if you decide to go to industry.</p> <h3 id="stress-culture">Stress Culture</h3> <p>Stress culture is present at CMU, and a significant portion of the students struggle with self-esteem and/or mental health issues. This is true at all elite institutions, especially within CS. However, both the difficulty of CMU’s computer science curriculum and the varied background knowledge of admitted students exacerbates the problem. The SCS core curriculum is doable for some students with sufficient background and very difficult for others. Stress culture is most prevalent in the earlier years of the undergraduate experience. As you develop your own interests within computer science and gain more experience, your confidence will increase and you will likely be much less affected by stress culture. Additionally, completing most of the CS core places most students on similar footing. Having a strong support system (e.g. friends you can relate to and seek comfort from) can also help mitigate the effects of stress.</p> <h3 id="less-startup-focused">Less Startup Focused</h3> <p>CMU is noticeably less startup focused than other elite computer science institutions. In a ranking system designed to keep track of the universities that produce the most entrepreneurs (Pitchbook’s 2020 University Report), Carnegie Mellon ranks 19. On the other hand, Stanford ranks 1, Berkeley ranks 2, and MIT ranks 3. However, this seems more attributed to the college culture rather than the opportunities given by CMU to create and develop startups. There are ways to get involved in entrepreneurship at CMU, including <a href="https://www.cmu.edu/swartz-center-for-entrepreneurship/education-and-resources/project-olympus/">Project Olympus</a>, a startup incubator program that helps students and faculty turn their ideas and research into startups. There are also classes you can take on entrepreneurship, listed <a href="https://www.cmu.edu/swartz-center-for-entrepreneurship/education-and-resources/courses-and-degrees.html">here</a>. It’s also important to note that despite its small size, the CMU startup community is very supportive, connected, and inclusive.</p> <h1 id="conclusion">Conclusion</h1> <p>I hope that this post gives you a lot more information about being a SCS student, and if you’re a high school senior, it helps make your college decision. Feel free to contact me if you have any questions or feedback!</p>]]></content><author><name></name></author><category term="cmu"/><summary type="html"><![CDATA[Choosing the right college to attend can be a really daunting experience, since there are so many things to consider! My hope is that this post gives some clarity as to what CMU SCS has to offer to undergrads (and what it doesn’t), and helps you make an informed decision.]]></summary></entry></feed>